{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computação Bioinspirada 2024-02\n",
    "\n",
    "Este repositório contém o trabalho da Tópicos Avançados em Ciências de Computação II\n",
    "(Computação Bioinspirada, no semestre 2024-02).\n",
    "\n",
    "O trabalho consiste em trabalhar com um conjunto de dados multirrótulo onde cada instância representa uma sequência de\n",
    "proteína. Cada rótulo (classe) corresponde a uma localização subcelular e as proteínas podem estar presentes\n",
    "simultaneamente em dois ou mais compartimentos celulares. O conjunto de dados possui seis localizações\n",
    "subcelulares: Proteínas do Capsídeo Viral, Proteínas da Membrana Celular do Hospedeiro, Proteínas do Retículo\n",
    "Endoplasmático do Hospedeiro, Proteínas do Citoplasma do Hospedeiro, Proteínas do Núcleo do Hospedeiro e\n",
    "Proteínas Secretadas. As colunas representam os códigos de Gene Ontology (relacionados à função da proteína),\n",
    "com valores que indicam a frequência do código para cada proteína. As seis últimas colunas indicam a presença\n",
    "(1) ou ausência (0) da proteína em cada uma das localizações subcelulares mencionadas.\n",
    "\n",
    "Desenvolvemos um modelo de classificação multirrótulo usando **redes neurais artificiais**.\n",
    "\n",
    "Existem dois conjuntos de dados: um de vírus e um de plantas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "tf.random.set_seed(random_seed)\n",
    "tf.keras.utils.set_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo os dados dos datasets e separando em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "plants_dataset = pd.read_csv('./Plants_Dataset_Term_Frequency.tsv', sep='\\t', skiprows=1).iloc[:, 1:]\n",
    "virus_dataset =  pd.read_csv('./Virus_Dataset_Term_Frequency.tsv', sep='\\t', skiprows=1).iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar as features e os rótulos para o dataset de plantas\n",
    "X_plants = plants_dataset.iloc[:, :-6].values  # Todas as colunas, exceto as últimas 6 e a primeira\n",
    "y_plants = plants_dataset.iloc[:, -6:].values  # As últimas 6 colunas (rótulos)\n",
    "\n",
    "# Separar as features e os rótulos para o dataset de vírus\n",
    "X_virus = virus_dataset.iloc[:, :-6].values\n",
    "y_virus = virus_dataset.iloc[:, -6:].values\n",
    "\n",
    "# Dividir dados em treino e teste\n",
    "X_train_plants, X_test_plants, y_train_plants, y_test_plants = train_test_split(\n",
    "    X_plants, y_plants, test_size=0.3, random_state=random_seed\n",
    ")\n",
    "\n",
    "X_train_virus, X_test_virus, y_train_virus, y_test_virus = train_test_split(\n",
    "    X_virus, y_virus, test_size=0.3, random_state=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Padronizar os dados\n",
    "scaler_plants = StandardScaler()\n",
    "X_train_plants = scaler_plants.fit_transform(X_train_plants)\n",
    "X_test_plants = scaler_plants.transform(X_test_plants)\n",
    "\n",
    "scaler_virus = StandardScaler()\n",
    "X_train_virus = scaler_virus.fit_transform(X_train_virus)\n",
    "X_test_virus = scaler_virus.transform(X_test_virus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(predictions, actual_values):\n",
    "    N = len(predictions)\n",
    "    L = 6\n",
    "\n",
    "    s = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(L):\n",
    "            s += predictions[i][j] ^ actual_values[i][j]\n",
    "    \n",
    "    return s / (N * L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, in_sz, out_sz, h_layer_num, features):\n",
    "        self.in_sz = in_sz\n",
    "        self.out_sz = out_sz\n",
    "        self.h_layer_num = h_layer_num\n",
    "        self.layer_features = features\n",
    "        self.model = None\n",
    "        self.hamming_loss = 0\n",
    "\n",
    "    def create_model(self):\n",
    "        model = None\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(self.in_sz,)))\n",
    "        for neur_num, act_func in zip(self.layer_features[0], self.layer_features[1]):\n",
    "            model.add(Dense(neur_num, activation=act_func))\n",
    "        model.add(Dense(self.out_sz, activation='sigmoid'))  # Saída com sigmoid para classificação binária\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        self.model = model\n",
    "    \n",
    "    def train_model(self, X_train, y_train, epochs, batch, val_split):\n",
    "        self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch, \n",
    "                       validation_split=val_split, verbose=0)\n",
    "    \n",
    "    def model_loss(self, X_test, y_test):\n",
    "        predictions_test = self.model.predict(X_test)\n",
    "        predicted_test = (predictions_test > 0.5).astype(int)\n",
    "        self.hamming_loss += hamming_loss(predicted_test, y_test)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar e treinar o modelo para plantas\n",
    "features = [[128, 64], ['relu', 'relu']]\n",
    "nn = NeuralNetwork(X_train_plants.shape[1], y_train_plants.shape[1], 3, features)\n",
    "nn.create_model()\n",
    "history_plants = nn.train_model(X_train_plants, y_train_plants, epochs=50, batch=32, val_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar e treinar o modelo para vírus\n",
    "nn2 = NeuralNetwork(X_train_virus.shape[1], y_train_virus.shape[1], 3, features)\n",
    "nn2.create_model()\n",
    "history_virus = nn2.train_model(X_train_virus, y_train_virus, epochs=50, batch=32, val_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n"
     ]
    }
   ],
   "source": [
    "# Obter previsões para o conjunto de teste de plantas\n",
    "predictions_plants_test = nn.model.predict(X_test_plants)\n",
    "\n",
    "# Obter previsões para o conjunto de treino de plantas\n",
    "predictions_plants_train = nn.model.predict(X_train_plants)\n",
    "\n",
    "# Obter previsões para o conjunto de teste de vírus\n",
    "predictions_virus_test = nn2.model.predict(X_test_virus)\n",
    "\n",
    "# Obter previsões para o conjunto de treino de vírus\n",
    "predictions_virus_train = nn2.model.predict(X_train_virus)\n",
    "\n",
    "\n",
    "# Transformar as probabilidades em rótulos binários (0 ou 1) com um limiar de 0.5\n",
    "predicted_classes_plants_test = (predictions_plants_test > 0.5).astype(int)\n",
    "predicted_classes_plants_train = (predictions_plants_train > 0.5).astype(int)\n",
    "predicted_classes_virus_test = (predictions_virus_test > 0.5).astype(int)\n",
    "predicted_classes_virus_train = (predictions_virus_train > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "possibilidades = [\n",
    "    [predicted_classes_plants_train, y_train_plants, 'dados de treino das plantas'],\n",
    "    [predicted_classes_plants_test, y_test_plants, 'dados de teste das plantas'],\n",
    "    [predicted_classes_virus_train, y_train_virus, 'dados de treino dos vírus'],\n",
    "    [predicted_classes_virus_test, y_test_virus, 'dados de teste dos vírus']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A hamming loss nos dados de treino das plantas foi de 1.16%\n",
      "A hamming loss nos dados de teste das plantas foi de 3.97%\n",
      "A hamming loss nos dados de treino dos vírus foi de 3.15%\n",
      "A hamming loss nos dados de teste dos vírus foi de 6.18%\n"
     ]
    }
   ],
   "source": [
    "for possibilidade in possibilidades:\n",
    "    print(f'A hamming loss nos {possibilidade[-1]} foi de {100*hamming_loss(*possibilidade[:-1]):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possíveis valores para nossa população inicial\n",
    "\n",
    "p_neur = [2**i for i in range(5, 10)]\n",
    "p_h_layers = [i for i in range(2, 5)]\n",
    "p_act_func = ['relu', 'gelu', 'leaky_relu', 'tanh']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gerando uma RN com parâmetros 'aleatórios' (dentro de opções pré definidas)\n",
    "\n",
    "def gen_random_nn():\n",
    "    layers = random.choice(p_h_layers)\n",
    "    act_funcs = [random.choice(p_act_func) for _ in range(layers)]\n",
    "    neur_num = [random.choice(p_neur) for _ in range(layers)]\n",
    "    nn = NeuralNetwork(X_train_plants.shape[1], y_train_plants.shape[1], layers, [neur_num, act_funcs])\n",
    "    return nn\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = gen_random_nn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que está feito -> \n",
    "Classe NeuralNetwork, que vai ser o indivíduo do nosso \n",
    "PSO\n",
    "\n",
    "## O que precisa fazer? \n",
    "O PSO.\n",
    "\n",
    "### Mais especificamente:\n",
    "\n",
    "1. Gerar uma população de 12 indivíduos (existem motivos para esse número*) \n",
    "aleatórios\n",
    "2. Inicializar a posição e velocidade\n",
    "3. Realizar o treinamento de cada indivíduo e analisar a perda de hamming como fitness\n",
    "4. Realizar a atualização da posição e velocidade de cada indivíduo\n",
    "5. Voltar para 3 (posição e velocidade mudarão os parâmetros dos ind)\n",
    "\n",
    "### Considerações sobre o algoritmo:\n",
    "\n",
    "1. Quantas vizinhanças? Definir as vizinhanças por N de camdas? São poucos indivíduos\n",
    "então seria computacionalmente barato iterar pela população\n",
    "2. O split nos dados de treino e teste vai ser feito a cada geração? Um para cada\n",
    "vizinhança, um para cada indivíduo?\n",
    "\n",
    "## Quais serão nossos hiperparâmetros? \n",
    "\n",
    "Número de Neurônios por camada [32-256] passo de 16\n",
    "\n",
    "Número de camadas escondidas [2-4]\n",
    "\n",
    "Tipo da função de ativação [relu, gelu, leakyrelu, tanh]\n",
    "\n",
    "## Considerações do que pode ser incluso como HPP\n",
    "\n",
    "Número de Épocas\n",
    "\n",
    "Tamanho do batch\n",
    "\n",
    "validation split\n",
    "\n",
    "??\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
